{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMqsGXL23FyHiaDoK/BrZSC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "214dc90a0e654400afc472016e0b7c9a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c2ce41185c3046948178700750de15b5",
              "IPY_MODEL_eb4c10cd3e8841a78e96cd4fa4024700",
              "IPY_MODEL_9150f3d3ea024b248be3f6bd0e51d57a"
            ],
            "layout": "IPY_MODEL_39d0d3b4c43f49e2b05a6e8f188a1c57"
          }
        },
        "c2ce41185c3046948178700750de15b5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8444b4869f6246b6bb77ce85d55fc75a",
            "placeholder": "​",
            "style": "IPY_MODEL_92bf67b384a445bc814c611c5e524c7a",
            "value": "100%"
          }
        },
        "eb4c10cd3e8841a78e96cd4fa4024700": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5c6cd07ab4bf408aaed547bc40a849d7",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f4209cb165154eadaab1c10da0ec1677",
            "value": 1
          }
        },
        "9150f3d3ea024b248be3f6bd0e51d57a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3a6b904c8a99465e90a2e2710a27191d",
            "placeholder": "​",
            "style": "IPY_MODEL_4942f87d85aa4a45b7c2c8f797b1f511",
            "value": " 1/1 [00:00&lt;00:00, 13.76it/s]"
          }
        },
        "39d0d3b4c43f49e2b05a6e8f188a1c57": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8444b4869f6246b6bb77ce85d55fc75a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "92bf67b384a445bc814c611c5e524c7a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5c6cd07ab4bf408aaed547bc40a849d7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f4209cb165154eadaab1c10da0ec1677": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "3a6b904c8a99465e90a2e2710a27191d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4942f87d85aa4a45b7c2c8f797b1f511": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/faraway1nspace/AnathemTransformer/blob/main/dev/notebooks/dev_anathem_transformer_base_layers.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Development Notebook: build and test base layers"
      ],
      "metadata": {
        "id": "5gcFatBIhzdu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Playing Around with novel architectures"
      ],
      "metadata": {
        "id": "0XS2jBKoMmMn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install torch transformers datasets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CQy_iTw-1g8O",
        "outputId": "19f6313e-1027-401e-c0a1-0e1f23ffa53a"
      },
      "execution_count": 174,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.0.1+cu118)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.29.2)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (2.12.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.12.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch) (16.0.5)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.14.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.27.1)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.13.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (9.0.0)\n",
            "Requirement already satisfied: dill<0.3.7,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.6)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.2.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.14)\n",
            "Requirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.4.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.8.4)\n",
            "Requirement already satisfied: responses<0.19 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.18.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.0.12)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2022.7.1)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModel, AutoTokenizer\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "from transformers.models.bert.modeling_bert import BertEncoder\n",
        "from transformers.activations import ACT2FN\n",
        "\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"distilroberta-base\")\n",
        "basemod = AutoModel.from_pretrained(\"distilroberta-base\")\n",
        "device = basemod.device"
      ],
      "metadata": {
        "id": "bCv855u5Mlgr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b705bfe8-f9ea-4f16-adda-d7b8b3938e0a"
      },
      "execution_count": 175,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at distilroberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.dense.bias']\n",
            "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dir(basemod)\n",
        "# base embedding layers\n",
        "layer_emb = copy.deepcopy(basemod._modules['embeddings'])\n"
      ],
      "metadata": {
        "id": "DToaqxfoM6bR"
      },
      "execution_count": 181,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# base trasnformers (full)\n",
        "layer_basetransformer = copy.deepcopy(basemod._modules['encoder']._modules['layer']._modules['0'])"
      ],
      "metadata": {
        "id": "ha_xqtWlNIfI"
      },
      "execution_count": 182,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# text \n",
        "text = [\n",
        "    \"A standard indemnity clause is a waiver clause that states that one party won't hold the other liable for damages, losses, or costs associated with legal issues1.\",\n",
        "    \"It usually consists of two elements: a trigger event or circumstance and a payment obligation2. The trigger event or circumstance is the breach of the agreement, willful misconduct, or negligence of the indemnifying party or its affiliates\"\n",
        "]\n",
        "\n",
        "import math\n",
        "\n",
        "#padding_length = int(math.ceil(max_length / 4)) * 4\n",
        "tokens = tokenizer(text,padding=True, return_tensors='pt')\n",
        "input_shape = tokens['input_ids'].size()\n",
        "\n",
        "# change token padding to be multiple of 4\n",
        "ideal_length = int(math.ceil(input_shape[-1] / 4)) * 4 # should be a multiple of 4\n",
        "if input_shape[-1]!=ideal_length:\n",
        "  tokens = tokenizer(text,padding='max_length', max_length = ideal_length, return_tensors='pt')\n",
        "  input_shape = tokens['input_ids'].size()\n",
        "\n",
        "token_type_ids = torch.zeros(input_shape, dtype=torch.long, device=device)\n",
        "tokens['token_type_ids'] = token_type_ids\n",
        "past_key_values_length =0\n",
        "\n",
        "# need to extend attention mask\n",
        "extended_attention_mask = basemod.get_extended_attention_mask(tokens['attention_mask'], input_shape)\n",
        "tokens['extended_attention_mask'] = extended_attention_mask\n",
        "print(tokens.keys())\n",
        "print(tokens['input_ids'].shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m9OMlauFNTPZ",
        "outputId": "ba5aef5a-fee2-4981-c25b-4c7c12c2ecda"
      },
      "execution_count": 183,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['input_ids', 'attention_mask', 'token_type_ids', 'extended_attention_mask'])\n",
            "torch.Size([2, 48])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "silo_dimensions = {0:basemod.config.hidden_size,\n",
        "                  1:basemod.config.hidden_size//2,\n",
        "                  2:basemod.config.hidden_size//4,\n",
        "                  }\n",
        "reintegration_dim = silo_dimensions[1] + silo_dimensions[2]\n"
      ],
      "metadata": {
        "id": "_PXX-SRjd7Mv"
      },
      "execution_count": 184,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_output = layer_emb(\n",
        "            input_ids=tokens['input_ids'],\n",
        "            position_ids=tokens.get('position_ids',None),\n",
        "            token_type_ids=tokens['token_type_ids'],\n",
        "            inputs_embeds=None,\n",
        "            past_key_values_length=past_key_values_length\n",
        ")\n",
        "print(embedding_output.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XuMY6iaRNpOc",
        "outputId": "e8aa93d7-72f8-48a6-ce18-c79fd0ad18c2"
      },
      "execution_count": 185,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 48, 768])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# basemodel transformer outputs: *full bert model\n",
        "out_l1 = layer_basetransformer(\n",
        "    hidden_states = embedding_output,\n",
        "    attention_mask = tokens['extended_attention_mask'],#tokens['attention_mask'],\n",
        "    head_mask=None,\n",
        "    encoder_hidden_states=None,\n",
        "    encoder_attention_mask=None,\n",
        "    #past_key_values=0,\n",
        "    #use_cache=None,\n",
        "    output_attentions=True,\n",
        "    #output_hidden_states=True,\n",
        "    #return_dict=True\n",
        ")\n",
        "\n",
        "hidden_states_l1 = out_l1[0]\n",
        "self_attention_l1 = out_l1[1]"
      ],
      "metadata": {
        "id": "3kgUzTJsO_1i"
      },
      "execution_count": 186,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Next Layer: \n",
        "# Query -> max pool and reduce  hidden dimension // 2\n",
        "# Key -> reduce hidden_dim // 2\n",
        "# value -> reduce hidden_dim //2\n",
        "#maxpool_l2 = nn.MaxPool2d((2,1), stride=None, padding=0, dilation=1, return_indices=False, ceil_mode=True)\n",
        "\n",
        "maxpool_l2 = nn.Sequential(\n",
        "    nn.Dropout(0.05),\n",
        "    nn.MaxPool2d((2,1), stride=None, padding=0, dilation=1, return_indices=False, ceil_mode=True),\n",
        ")\n",
        "\n",
        "maxpool_l2_attn = nn.MaxPool1d((2), stride=None, padding=0, dilation=1, return_indices=False, ceil_mode=True)"
      ],
      "metadata": {
        "id": "cvj_XlNsTYXt"
      },
      "execution_count": 187,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# reduce dimension of hidden states\n",
        "hiddens_states_l1_reduced = maxpool_l2(hidden_states_l1)\n",
        "print(hidden_states_l1.shape)\n",
        "print(hiddens_states_l1_reduced.shape)\n",
        "\n",
        "# reduce dimension of attention mask\n",
        "attention_mask_l1_reduced = maxpool_l2_attn(tokens['attention_mask'].float())\n",
        "print(attention_mask_l1_reduced.shape)\n",
        "\n",
        "# extend the dimension of the reduced attention_mask\n",
        "print(input_shape)\n",
        "extended_attention_mask_l1_reduced = basemod.get_extended_attention_mask(attention_mask_l1_reduced, attention_mask_l1_reduced.shape)\n",
        "print(tokens['extended_attention_mask'].shape)\n",
        "print(extended_attention_mask_l1_reduced.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PJ5xAaELVuwk",
        "outputId": "9b978604-9599-4093-8ab7-284a31967435"
      },
      "execution_count": 188,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 48, 768])\n",
            "torch.Size([2, 24, 768])\n",
            "torch.Size([2, 24])\n",
            "torch.Size([2, 48])\n",
            "torch.Size([2, 1, 1, 48])\n",
            "torch.Size([2, 1, 1, 24])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Try to do Multi Headed attenion with differently sized query and value "
      ],
      "metadata": {
        "id": "-AcdIY3shHWN"
      },
      "execution_count": 189,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import math\n",
        "from typing import Optional, Tuple\n",
        "import copy\n",
        "\n",
        "class BertSelfAttnDimensionReduction(nn.Module):\n",
        "    \"\"\"Bert Attention Layer that uses a dimension-reduced version of the query, so to reduce the dimension of the outputs\"\"\"\n",
        "    def __init__(\n",
        "        self, \n",
        "        config, \n",
        "        hidden_size_input=768, \n",
        "        hidden_size_query = None,\n",
        "        position_embedding_type=None, \n",
        "        dim_reduction = 2\n",
        "    ):\n",
        "        \"\"\"Special type of Bert Self attention that reduces the dimension of the inputs by half\"\"\"\n",
        "        super().__init__()\n",
        "        if (config.hidden_size // dim_reduction) % config.num_attention_heads != 0 and not hasattr(config, \"embedding_size\"):\n",
        "            raise ValueError(\n",
        "                f\"The hidden size ({config.hidden_size}) is not a multiple of the number of attention \"\n",
        "                f\"heads ({config.num_attention_heads})\"\n",
        "            )\n",
        "        self.dim_reduction = dim_reduction\n",
        "        self.hidden_size_input = hidden_size_input\n",
        "        self.hidden_size_reduced = hidden_size_input // dim_reduction\n",
        "        if hidden_size_query is None:\n",
        "            hidden_size_query = hidden_size_input\n",
        "        self.hidden_size_query = hidden_size_query\n",
        "        self.num_attention_heads = config.num_attention_heads\n",
        "        self.attention_head_size = int(self.hidden_size_reduced / config.num_attention_heads)\n",
        "        self.all_head_size = self.num_attention_heads * self.attention_head_size\n",
        "\n",
        "        self.query = nn.Linear(self.hidden_size_query, self.all_head_size)\n",
        "        self.key = nn.Linear(self.hidden_size_input, self.all_head_size)\n",
        "        self.value = nn.Linear(self.hidden_size_input, self.all_head_size)\n",
        "\n",
        "        self.dropout = nn.Dropout(config.attention_probs_dropout_prob)\n",
        "        self.position_embedding_type = position_embedding_type or getattr(\n",
        "            config, \"position_embedding_type\", \"absolute\"\n",
        "        )\n",
        "        if self.position_embedding_type == \"relative_key\" or self.position_embedding_type == \"relative_key_query\":\n",
        "            self.max_position_embeddings = config.max_position_embeddings\n",
        "            self.distance_embedding = nn.Embedding(2 * config.max_position_embeddings - 1, self.attention_head_size)\n",
        "\n",
        "        self.is_decoder = config.is_decoder\n",
        "\n",
        "    def transpose_for_scores(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)\n",
        "        x = x.view(new_x_shape)\n",
        "        return x.permute(0, 2, 1, 3)\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        hidden_states: torch.Tensor,\n",
        "        attention_mask: Optional[torch.FloatTensor] = None,\n",
        "        head_mask: Optional[torch.FloatTensor] = None,\n",
        "        encoder_hidden_states: Optional[torch.FloatTensor] = None,\n",
        "        encoder_attention_mask: Optional[torch.FloatTensor] = None,\n",
        "        past_key_value: Optional[Tuple[Tuple[torch.FloatTensor]]] = None,\n",
        "        output_attentions: Optional[bool] = False,\n",
        "    ) -> Tuple[torch.Tensor]:\n",
        "        mixed_query_layer = self.query(hidden_states)\n",
        "\n",
        "        # If this is instantiated as a cross-attention module, the keys\n",
        "        # and values come from an encoder; the attention mask needs to be\n",
        "        # such that the encoder's padding tokens are not attended to.\n",
        "\n",
        "        key_layer = self.transpose_for_scores(self.key(encoder_hidden_states))\n",
        "        value_layer = self.transpose_for_scores(self.value(encoder_hidden_states))\n",
        "        query_layer = self.transpose_for_scores(mixed_query_layer)\n",
        "\n",
        "        # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n",
        "        attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n",
        "\n",
        "        if self.position_embedding_type == \"relative_key\" or self.position_embedding_type == \"relative_key_query\":\n",
        "            query_length, key_length = query_layer.shape[2], key_layer.shape[2]\n",
        "            if use_cache:\n",
        "                position_ids_l = torch.tensor(key_length - 1, dtype=torch.long, device=hidden_states.device).view(\n",
        "                    -1, 1\n",
        "                )\n",
        "            else:\n",
        "                position_ids_l = torch.arange(query_length, dtype=torch.long, device=hidden_states.device).view(-1, 1)\n",
        "            position_ids_r = torch.arange(key_length, dtype=torch.long, device=hidden_states.device).view(1, -1)\n",
        "            distance = position_ids_l - position_ids_r\n",
        "\n",
        "            positional_embedding = self.distance_embedding(distance + self.max_position_embeddings - 1)\n",
        "            positional_embedding = positional_embedding.to(dtype=query_layer.dtype)  # fp16 compatibility\n",
        "\n",
        "            if self.position_embedding_type == \"relative_key\":\n",
        "                relative_position_scores = torch.einsum(\"bhld,lrd->bhlr\", query_layer, positional_embedding)\n",
        "                attention_scores = attention_scores + relative_position_scores\n",
        "            elif self.position_embedding_type == \"relative_key_query\":\n",
        "                relative_position_scores_query = torch.einsum(\"bhld,lrd->bhlr\", query_layer, positional_embedding)\n",
        "                relative_position_scores_key = torch.einsum(\"bhrd,lrd->bhlr\", key_layer, positional_embedding)\n",
        "                attention_scores = attention_scores + relative_position_scores_query + relative_position_scores_key\n",
        "\n",
        "        attention_scores = attention_scores / math.sqrt(self.attention_head_size)\n",
        "        if encoder_attention_mask is not None:\n",
        "            # Apply the attention mask is (precomputed for all layers in BertModel forward() function)\n",
        "            #print(attention_scores.shape)\n",
        "            #print(attention_scores.shape)\n",
        "            attention_scores = attention_scores + encoder_attention_mask\n",
        "\n",
        "        # Normalize the attention scores to probabilities.\n",
        "        attention_probs = nn.functional.softmax(attention_scores, dim=-1)\n",
        "\n",
        "        # This is actually dropping out entire tokens to attend to, which might\n",
        "        # seem a bit unusual, but is taken from the original Transformer paper.\n",
        "        attention_probs = self.dropout(attention_probs)\n",
        "\n",
        "        # Mask heads if we want to\n",
        "        if head_mask is not None:\n",
        "            attention_probs = attention_probs * head_mask\n",
        "\n",
        "        context_layer = torch.matmul(attention_probs, value_layer)\n",
        "\n",
        "        context_layer = context_layer.permute(0, 2, 1, 3).contiguous()\n",
        "        new_context_layer_shape = context_layer.size()[:-2] + (self.all_head_size,)\n",
        "        context_layer = context_layer.view(new_context_layer_shape)\n",
        "\n",
        "        outputs = (context_layer, attention_probs) if output_attentions else (context_layer,)\n",
        "\n",
        "        if self.is_decoder:\n",
        "            outputs = outputs + (past_key_value,)\n",
        "        return outputs\n",
        "\n",
        "bertlayer_l2_reduction = BertSelfAttnDimensionReduction(\n",
        "    config=basemod.config,\n",
        "    hidden_size_input=basemod.config.hidden_size, \n",
        "    position_embedding_type=basemod.config.position_embedding_type,\n",
        "    dim_reduction = 2\n",
        ")\n",
        "\n",
        "bertlayer_l3_reduction = BertSelfAttnDimensionReduction(\n",
        "    config=basemod.config,\n",
        "    hidden_size_input=basemod.config.hidden_size // 2, \n",
        "    position_embedding_type=basemod.config.position_embedding_type,\n",
        "    dim_reduction = 2\n",
        ")"
      ],
      "metadata": {
        "id": "oYHtEQNFgh7U"
      },
      "execution_count": 214,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "out_l2 = bertlayer_l2_reduction(\n",
        "        hidden_states = hiddens_states_l1_reduced,\n",
        "        attention_mask = extended_attention_mask_l1_reduced,\n",
        "        head_mask=None,\n",
        "        encoder_hidden_states = hidden_states_l1,\n",
        "        encoder_attention_mask= tokens['extended_attention_mask'],\n",
        "        past_key_value=None,\n",
        "        output_attentions=False\n",
        "    )\n",
        "hidden_states_l2 = out_l2[0]\n",
        "print(hidden_states_l2.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "03mPp6aHgh9Y",
        "outputId": "9f2e3e0b-9576-44ae-cba4-324890db78b4"
      },
      "execution_count": 215,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 24, 384])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Next dimension reduction: \n",
        "hiddens_states_l2_reduced = maxpool_l2(hidden_states_l2)\n",
        "print(hidden_states_l2.shape)\n",
        "print(hiddens_states_l2_reduced.shape)\n",
        "\n",
        "# reduce dimension of attention mask\n",
        "attention_mask_l2_reduced = maxpool_l2_attn(attention_mask_l1_reduced.float())\n",
        "print(attention_mask_l2_reduced.shape)\n",
        "\n",
        "# extend the dimension of the reduced attention_mask\n",
        "extended_attention_mask_l2_reduced = basemod.get_extended_attention_mask(attention_mask_l2_reduced, attention_mask_l2_reduced.shape)\n",
        "print(extended_attention_mask_l2_reduced.shape)\n",
        "\n",
        "if True:\n",
        "  out_l3 = bertlayer_l3_reduction(\n",
        "        hidden_states = hiddens_states_l2_reduced, # input has been maxpooled\n",
        "        attention_mask = extended_attention_mask_l2_reduced,\n",
        "        head_mask=None,\n",
        "        encoder_hidden_states = hidden_states_l2,\n",
        "        encoder_attention_mask= extended_attention_mask_l1_reduced,\n",
        "        past_key_value=None,\n",
        "        output_attentions=False\n",
        "    )\n",
        "  hidden_states_l3 = out_l3[0]\n",
        "  print(hidden_states_l3.shape)\n",
        "\n",
        "\n",
        "# The outputs of the bertlayer_l3_reduction can now run through a usual BertLayer for 3 times"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hlN5aOsYgh_z",
        "outputId": "5cb9c9a7-d930-4c14-b4b0-fd69f028d1dc"
      },
      "execution_count": 192,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 24, 384])\n",
            "torch.Size([2, 12, 384])\n",
            "torch.Size([2, 12])\n",
            "torch.Size([2, 1, 1, 12])\n",
            "torch.Size([2, 12, 192])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# The outputs of the bertlayer_l3_reduction can now run through a usual BertLayer for 3 times\n",
        "\n",
        "config_lowres_encoder = copy.deepcopy(basemod.config)\n",
        "config_lowres_encoder.hidden_size = config_lowres_encoder.hidden_size//4\n",
        "config_lowres_encoder.num_hidden_layers = 3\n",
        "print(config_lowres_encoder)\n",
        "\n",
        "# The outputs of the bertlayer_l3_reduction can now run through a usual BertLayer for 3 times\n",
        "encoder_lowres = BertEncoder(config_lowres_encoder)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6q-GTg2fgiCB",
        "outputId": "7237deaf-676e-4481-934b-dde2eeadc287"
      },
      "execution_count": 193,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RobertaConfig {\n",
            "  \"_name_or_path\": \"distilroberta-base\",\n",
            "  \"architectures\": [\n",
            "    \"RobertaForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 192,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 3,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.29.2\",\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "out_encoder_lowres = encoder_lowres(\n",
        "    hidden_states=hidden_states_l3,\n",
        "    attention_mask=extended_attention_mask_l2_reduced,\n",
        "    head_mask = None,\n",
        "    return_dict=True,\n",
        ")\n",
        "hidden_states_lowres = out_encoder_lowres[0]\n",
        "print(hidden_states_lowres.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uTIN07S24_qp",
        "outputId": "873a2658-70fe-4b5f-c971-adb49c8ed338"
      },
      "execution_count": 194,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 12, 192])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Upresolution Layer: up-resolution from dim-3 to dim-2 is as follows:\n",
        "# hs_l3 -> upsampled sequence-length as hs-l2\n",
        "# -> could have another attention-based mechanism that expands dimension of hs-l2\n",
        "\n",
        "class InterpolateCombo(nn.Module):\n",
        "    \"\"\"there could also be an attentive way to do this\"\"\"\n",
        "    def __init__(self, scale_factor=2, dropout=0.05, alpha=0.667):\n",
        "        \"\"\"Arguments:\n",
        "        :param scaler_factor: float, multiple of up-scaling\n",
        "        :param dropout: float, dropout proportion\n",
        "        :param alpha: float, mixture weight between nearest-neighbor vs linear-interpolation\n",
        "        \"\"\"\n",
        "        super(InterpolateCombo, self).__init__()\n",
        "        self.interp = nn.functional.interpolate\n",
        "        self.scale_factor = scale_factor\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.a = alpha\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x_trans = x.transpose(-2,-1)\n",
        "        z = self.a*self.interp(x_trans, mode='nearest',scale_factor=self.scale_factor) + (1-self.a)*self.interp(x_trans, mode='linear',scale_factor=self.scale_factor)\n",
        "        z = self.dropout(z)\n",
        "        return z.transpose(-2,-1)\n",
        "\n",
        "#hidden_states_upscaled_3to2_nearest = nn.functional.interpolate(hidden_states_rowres.transpose(-2,-1), scale_factor=2, mode='nearest').transpose(-2,-1)\n",
        "#hidden_states_upscaled_3to2_linear = nn.functional.interpolate(hidden_states_rowres.transpose(-2,-1), scale_factor=2, mode='linear').transpose(-2,-1)\n",
        "\n",
        "upscaler_x2 = InterpolateCombo(scale_factor=2)"
      ],
      "metadata": {
        "id": "BM75xap8L5cB"
      },
      "execution_count": 195,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hidden_states_upscaled3to2 = upscaler_x2(hidden_states_lowres)\n"
      ],
      "metadata": {
        "id": "Lfpn9mEsPPCf"
      },
      "execution_count": 197,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## BertAttentiveIntegrator\n",
        "\n",
        "class BertCrossAttention(nn.Module):\n",
        "    def __init__(\n",
        "        self, \n",
        "        config, \n",
        "        hidden_size,\n",
        "        hidden_size_query,\n",
        "        position_embedding_type=None\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.hidden_size_query = hidden_size_query\n",
        "        if self.hidden_size % config.num_attention_heads != 0 and not hasattr(config, \"embedding_size\"):\n",
        "            raise ValueError(\n",
        "                f\"The hidden size ({self.hidden_size}) is not a multiple of the number of attention \"\n",
        "                f\"heads ({config.num_attention_heads})\"\n",
        "            )\n",
        "\n",
        "        self.num_attention_heads = config.num_attention_heads\n",
        "        self.attention_head_size = int(self.hidden_size / config.num_attention_heads)\n",
        "        self.all_head_size = self.num_attention_heads * self.attention_head_size\n",
        "\n",
        "        self.query = nn.Linear(self.hidden_size_query, self.all_head_size)\n",
        "        self.key = nn.Linear(self.hidden_size, self.all_head_size)\n",
        "        self.value = nn.Linear(self.hidden_size, self.all_head_size)\n",
        "\n",
        "        self.dropout = nn.Dropout(config.attention_probs_dropout_prob)\n",
        "        self.position_embedding_type = position_embedding_type or getattr(\n",
        "            config, \"position_embedding_type\", \"absolute\"\n",
        "        )\n",
        "        if self.position_embedding_type == \"relative_key\" or self.position_embedding_type == \"relative_key_query\":\n",
        "            self.max_position_embeddings = config.max_position_embeddings\n",
        "            self.distance_embedding = nn.Embedding(2 * config.max_position_embeddings - 1, self.attention_head_size)\n",
        "\n",
        "        self.is_decoder = config.is_decoder\n",
        "\n",
        "    def transpose_for_scores(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)\n",
        "        x = x.view(new_x_shape)\n",
        "        return x.permute(0, 2, 1, 3)\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        hidden_states: torch.Tensor,\n",
        "        attention_mask: Optional[torch.FloatTensor] = None,\n",
        "        head_mask: Optional[torch.FloatTensor] = None,\n",
        "        query_hidden_states: Optional[torch.FloatTensor] = None,\n",
        "        query_attention_mask: Optional[torch.FloatTensor] = None,\n",
        "        past_key_value: Optional[Tuple[Tuple[torch.FloatTensor]]] = None,\n",
        "        output_attentions: Optional[bool] = False,\n",
        "    ) -> Tuple[torch.Tensor]:\n",
        "        mixed_query_layer = self.query(query_hidden_states)\n",
        "\n",
        "        # If this is instantiated as a cross-attention module, the keys\n",
        "        # and values come from an encoder; the attention mask needs to be\n",
        "        # such that the encoder's padding tokens are not attended to.\n",
        "        key_layer = self.transpose_for_scores(self.key(hidden_states))\n",
        "        value_layer = self.transpose_for_scores(self.value(hidden_states))\n",
        "        query_layer = self.transpose_for_scores(mixed_query_layer)\n",
        "        \n",
        "        use_cache = past_key_value is not None\n",
        "        if self.is_decoder:\n",
        "            # if cross_attention save Tuple(torch.Tensor, torch.Tensor) of all cross attention key/value_states.\n",
        "            # Further calls to cross_attention layer can then reuse all cross-attention\n",
        "            # key/value_states (first \"if\" case)\n",
        "            # if uni-directional self-attention (decoder) save Tuple(torch.Tensor, torch.Tensor) of\n",
        "            # all previous decoder key/value_states. Further calls to uni-directional self-attention\n",
        "            # can concat previous decoder key/value_states to current projected key/value_states (third \"elif\" case)\n",
        "            # if encoder bi-directional self-attention `past_key_value` is always `None`\n",
        "            past_key_value = (key_layer, value_layer)\n",
        "\n",
        "        # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n",
        "        attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n",
        "\n",
        "        if self.position_embedding_type == \"relative_key\" or self.position_embedding_type == \"relative_key_query\":\n",
        "            query_length, key_length = query_layer.shape[2], key_layer.shape[2]\n",
        "            if use_cache:\n",
        "                position_ids_l = torch.tensor(key_length - 1, dtype=torch.long, device=hidden_states.device).view(\n",
        "                    -1, 1\n",
        "                )\n",
        "            else:\n",
        "                position_ids_l = torch.arange(query_length, dtype=torch.long, device=hidden_states.device).view(-1, 1)\n",
        "            position_ids_r = torch.arange(key_length, dtype=torch.long, device=hidden_states.device).view(1, -1)\n",
        "            distance = position_ids_l - position_ids_r\n",
        "\n",
        "            positional_embedding = self.distance_embedding(distance + self.max_position_embeddings - 1)\n",
        "            positional_embedding = positional_embedding.to(dtype=query_layer.dtype)  # fp16 compatibility\n",
        "\n",
        "            if self.position_embedding_type == \"relative_key\":\n",
        "                relative_position_scores = torch.einsum(\"bhld,lrd->bhlr\", query_layer, positional_embedding)\n",
        "                attention_scores = attention_scores + relative_position_scores\n",
        "            elif self.position_embedding_type == \"relative_key_query\":\n",
        "                relative_position_scores_query = torch.einsum(\"bhld,lrd->bhlr\", query_layer, positional_embedding)\n",
        "                relative_position_scores_key = torch.einsum(\"bhrd,lrd->bhlr\", key_layer, positional_embedding)\n",
        "                attention_scores = attention_scores + relative_position_scores_query + relative_position_scores_key\n",
        "\n",
        "        attention_scores = attention_scores / math.sqrt(self.attention_head_size)\n",
        "        if attention_mask is not None:\n",
        "            # Apply the attention mask is (precomputed for all layers in BertModel forward() function)\n",
        "            attention_scores = attention_scores + attention_mask\n",
        "\n",
        "        # Normalize the attention scores to probabilities.\n",
        "        attention_probs = nn.functional.softmax(attention_scores, dim=-1)\n",
        "\n",
        "        # This is actually dropping out entire tokens to attend to, which might\n",
        "        # seem a bit unusual, but is taken from the original Transformer paper.\n",
        "        attention_probs = self.dropout(attention_probs)\n",
        "\n",
        "        # Mask heads if we want to\n",
        "        if head_mask is not None:\n",
        "            attention_probs = attention_probs * head_mask\n",
        "\n",
        "        context_layer = torch.matmul(attention_probs, value_layer)\n",
        "\n",
        "        context_layer = context_layer.permute(0, 2, 1, 3).contiguous()\n",
        "        new_context_layer_shape = context_layer.size()[:-2] + (self.all_head_size,)\n",
        "        context_layer = context_layer.view(new_context_layer_shape)\n",
        "\n",
        "        outputs = (context_layer, attention_probs) if output_attentions else (context_layer,)\n",
        "\n",
        "        if self.is_decoder:\n",
        "            outputs = outputs + (past_key_value,)\n",
        "        return outputs"
      ],
      "metadata": {
        "id": "4GXB-9waPBv_"
      },
      "execution_count": 198,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bertlayer_l3_to_l2_crossattn = BertCrossAttention(\n",
        "        config=basemod.config, \n",
        "        hidden_size=silo_dimensions[1],\n",
        "        hidden_size_query=silo_dimensions[2],\n",
        "        position_embedding_type=None\n",
        "    )"
      ],
      "metadata": {
        "id": "edzAlaOIPDa5"
      },
      "execution_count": 199,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(hidden_states_upscaled3to2.shape)\n",
        "print(hidden_states_l2.shape)\n",
        "print(attention_mask_l1_reduced.shape)\n",
        "print(extended_attention_mask_l1_reduced.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CMmeXSBoipPv",
        "outputId": "10e4aa92-dacf-485c-f451-cb822f8ca9df"
      },
      "execution_count": 200,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 24, 192])\n",
            "torch.Size([2, 24, 384])\n",
            "torch.Size([2, 24])\n",
            "torch.Size([2, 1, 1, 24])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "out_l2_postencode = bertlayer_l3_to_l2_crossattn(\n",
        "    hidden_states = hidden_states_l2,\n",
        "    attention_mask = extended_attention_mask_l1_reduced,\n",
        "    head_mask = None,\n",
        "    query_hidden_states = hidden_states_upscaled3to2,\n",
        "    query_attention_mask = attention_mask_l1_reduced\n",
        ")\n",
        "hidden_states_l2_postencode = out_l2_postencode[0]\n",
        "print(hidden_states_l2_postencode.shape)\n",
        "assert hidden_states_l2_postencode.shape == hidden_states_l2.shape"
      ],
      "metadata": {
        "id": "JVMll5RTQyVh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "862a9cc9-16ae-4bcc-bbd2-51aabbb83975"
      },
      "execution_count": 201,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 24, 384])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(basemod.config.hidden_size)\n",
        "print(basemod.config.intermediate_size)\n",
        "print(basemod.config.intermediate_size/basemod.config.hidden_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Az5HD6Rc7POU",
        "outputId": "594d04bd-8c34-461d-e6e8-a5046c4eae7c"
      },
      "execution_count": 202,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "768\n",
            "3072\n",
            "4.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# how does bert actually work?\n",
        "\"\"\"\n",
        "input = x\n",
        "\n",
        "BertLayer:\n",
        "- BertAttention\n",
        "--- x2 = BertSelfAttention(x)\n",
        "--- x3 = BertSelfOutput(x2,x) -> lnorm(drop(f(x2)) + x)\n",
        "- BertIntermediate (expension:  4*hidden_size)\n",
        "--- x4_ex = activation(f(x3)) # expansion (4*)\n",
        "- BertOutput\n",
        "--- x5 = lnorm(drop(f(x4_ex)) + x3 )\n",
        "\n",
        "\n",
        "inputs = x_l2, x_l3_up\n",
        "\n",
        "BertIntegrativeLayer:\n",
        "- x2 = BertCrossAttention(k,v=x_l2, q=x_l3_up)\n",
        "- x3 = lnorm(drop(f(x2)) + x_l2)\n",
        "- x4_ex = activation( f(cat(x3, x_l3_up))  )\n",
        "- x5 = lnorm(drop(f(x4_ex)) + x3)\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "class BertIntegrativeLayer(nn.Module):\n",
        "    \"\"\"Vanilla Bert Layer, but integrates other hiddens states from a parallel transformers stack typically low-re\"\"\"\n",
        "    def __init__(\n",
        "            self, \n",
        "            config, \n",
        "            hidden_size, \n",
        "            hidden_size_query,\n",
        "            intermediate_size=None\n",
        "        ):\n",
        "        super().__init__()\n",
        "        #self.chunk_size_feed_forward = config.chunk_size_feed_forward\n",
        "        #self.seq_len_dim = 1\n",
        "        self.cat = torch.cat\n",
        "        if intermediate_size is None:\n",
        "            intermediate_size = int(4*hidden_size)\n",
        "        self.intermediate_size = intermediate_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.hidden_size_query = hidden_size_query\n",
        "        self.hidden_size_concat = int(hidden_size + hidden_size_query)\n",
        "\n",
        "        # cross attention between (low-res) query and hidden layers below\n",
        "        self.attention = BertCrossAttention(\n",
        "            config, \n",
        "            hidden_size,\n",
        "            hidden_size_query,\n",
        "            position_embedding_type=\"absolute\"\n",
        "        )\n",
        "        self.is_decoder = config.is_decoder\n",
        "        #self.intermediate = BertIntermediate(config)\n",
        "        #self.output = BertOutput(config)\n",
        "        #- x2 = BertCrossAttention(k,v=x_l2, q=x_l3_up)\n",
        "        #- x3 = lnorm(drop(f(x2)) + x_l2)\n",
        "        #- x4_ex = activation( f(cat(x3, x_l3_up))  )\n",
        "        #- x5 = lnorm(drop(f(x4_ex)) + x3)\n",
        "\n",
        "        # corresponds to BertAttention SelfOutput\n",
        "        self.output_attn = nn.Linear(self.hidden_size, self.hidden_size)\n",
        "        self.lnorm_attn = nn.LayerNorm(self.hidden_size, eps=config.layer_norm_eps)\n",
        "        self.dropout_attn = nn.Dropout(config.hidden_dropout_prob)\n",
        "\n",
        "        # corresponds to BertIntermediate\n",
        "        self.intermediate = nn.Linear(self.hidden_size_concat, self.intermediate_size)\n",
        "        if isinstance(config.hidden_act, str):\n",
        "            self.intermediate_act_fn = ACT2FN[config.hidden_act]\n",
        "        else:\n",
        "            self.intermediate_act_fn = config.hidden_act\n",
        "\n",
        "        # corresponds to BertOutput\n",
        "        self.output_intm = nn.Linear(self.intermediate_size, self.hidden_size)\n",
        "        self.lnorm_intm = nn.LayerNorm(self.hidden_size, eps=config.layer_norm_eps)\n",
        "        self.dropout_intm = nn.Dropout(config.hidden_dropout_prob)\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        hidden_states: torch.Tensor,\n",
        "        attention_mask: Optional[torch.FloatTensor] = None,\n",
        "        head_mask: Optional[torch.FloatTensor] = None,\n",
        "        query_hidden_states: Optional[torch.FloatTensor] = None,\n",
        "        query_attention_mask: Optional[torch.FloatTensor] = None,\n",
        "        past_key_value: Optional[Tuple[Tuple[torch.FloatTensor]]] = None,\n",
        "        output_attentions: Optional[bool] = False,\n",
        "    ) -> Tuple[torch.Tensor]:\n",
        "        # decoder uni-directional self-attention cached key/values tuple is at positions 1,2\n",
        "        self_attn_past_key_value = past_key_value[:2] if past_key_value is not None else None\n",
        "\n",
        "        # cross attn between hiddens states and (low-res) query vector\n",
        "        cross_attn_outputs = self.attention(\n",
        "            hidden_states = hidden_states,\n",
        "            attention_mask = attention_mask,\n",
        "            head_mask = head_mask,\n",
        "            query_hidden_states = query_hidden_states,\n",
        "            query_attention_mask = query_attention_mask\n",
        "        )\n",
        "        cross_hidden_states = cross_attn_outputs[0]\n",
        "\n",
        "        # first Add+Norm skip connection (BertSelfOutput)\n",
        "        cross_hidden_states = self.dropout_attn(self.output_attn(cross_hidden_states))\n",
        "        hidden_states = self.lnorm_attn(cross_hidden_states + hidden_states)\n",
        "\n",
        "        # intermediate expension\n",
        "        intermediate_states = self.intermediate_act_fn(self.intermediate(\n",
        "            self.cat((hidden_states, query_hidden_states),axis=2)\n",
        "        ))\n",
        "        assert intermediate_states.shape[0]==hidden_states.shape[0]\n",
        "        assert intermediate_states.shape[1]==hidden_states.shape[1]\n",
        "\n",
        "        # BertOutput\n",
        "        intermediate_states = self.dropout_intm(self.output_intm(intermediate_states))\n",
        "        out_states = self.lnorm_intm(intermediate_states + hidden_states)\n",
        "\n",
        "        #- x2 = BertCrossAttention(k,v=x_l2, q=x_l3_up)\n",
        "        #- x3 = lnorm(drop(f(x2)) + x_l2)\n",
        "        #- x4_ex = activation( f(cat(x3, x_l3_up))  )\n",
        "        #- x5 = lnorm(drop(f(x4_ex)) + x3)\n",
        "        return out_states\n"
      ],
      "metadata": {
        "id": "v_ElURFHWk3B"
      },
      "execution_count": 203,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# from low-res to mid-res\n",
        "bert_integrative_layer_midres = BertIntegrativeLayer(\n",
        "    basemod.config, \n",
        "    hidden_size=silo_dimensions[1], \n",
        "    hidden_size_query=silo_dimensions[2],\n",
        "    intermediate_size=silo_dimensions[1]*4,\n",
        ")\n",
        "\n",
        "# from mid-res to high-res\n",
        "bert_integrative_layer_hires = BertIntegrativeLayer(\n",
        "    basemod.config, \n",
        "    hidden_size=silo_dimensions[0], \n",
        "    hidden_size_query=reintegration_dim,\n",
        "    intermediate_size=silo_dimensions[0]*4,\n",
        ")"
      ],
      "metadata": {
        "id": "QeJysFTAgZm_"
      },
      "execution_count": 204,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hidden_states_midres = bert_integrative_layer_midres(\n",
        "    hidden_states = hidden_states_l2,\n",
        "    attention_mask = extended_attention_mask_l1_reduced,\n",
        "    head_mask = None,\n",
        "    query_hidden_states = hidden_states_upscaled3to2,\n",
        "    query_attention_mask = attention_mask_l1_reduced\n",
        ")\n",
        "print(hidden_states_midres.shape)\n",
        "assert hidden_states_midres.shape == hidden_states_l2.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xcZ3MU-4hFN3",
        "outputId": "6479ba55-8220-4957-e2e6-b5cd4272cf4f"
      },
      "execution_count": 205,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 24, 384])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# upscale the l2 and l3 to the full dimension\n",
        "upscaler_x4 = InterpolateCombo(scale_factor=4)\n",
        "hidden_states_upscaled3to1 = upscaler_x4(hidden_states_lowres)\n",
        "hidden_states_upscaled2to1 = upscaler_x2(hidden_states_midres)\n",
        "\n",
        "hidden_states_upscaled = torch.cat(\n",
        "    (hidden_states_upscaled2to1, hidden_states_upscaled3to1),\n",
        "    axis=2)\n",
        "\n",
        "print(hidden_states_upscaled.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iavtqpUohJAs",
        "outputId": "04e78c53-0218-4877-9952-a253a4b6e20d"
      },
      "execution_count": 206,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 48, 576])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# final layer to bring it up to full dimension\n",
        "hidden_states_hires = bert_integrative_layer_hires(\n",
        "    hidden_states = hidden_states_l1,\n",
        "    attention_mask = extended_attention_mask,\n",
        "    head_mask = None,\n",
        "    query_hidden_states = hidden_states_upscaled,\n",
        "    query_attention_mask = extended_attention_mask\n",
        ")\n",
        "print(hidden_states_hires.shape)\n",
        "assert hidden_states_hires.shape == hidden_states_l1.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5gDwXueYhJ1B",
        "outputId": "48d0b9c3-45fd-42a6-eb9b-b970039d5f23"
      },
      "execution_count": 207,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 48, 768])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "hidden_states_hires.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wv4ZVcYORoL_",
        "outputId": "cbf9b38b-3060-4849-8f73-64086769ac2f"
      },
      "execution_count": 208,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([2, 48, 768])"
            ]
          },
          "metadata": {},
          "execution_count": 208
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "attention_mask_l1_reduced.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QJQJVvdMle3v",
        "outputId": "7242543c-0481-4b42-8f9f-98bb0ef95cd3"
      },
      "execution_count": 210,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([2, 24])"
            ]
          },
          "metadata": {},
          "execution_count": 210
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### The Reduce and Integrate layer:\n",
        "- this is like a Transformer block, but:\n",
        "- does dimension reduction along sequence and embedding-dim\n",
        "- includes a skip connection from previous hidden-states of the same dimension"
      ],
      "metadata": {
        "id": "A8Z0KGMfm-Mn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "# this is the layer that just does cross-attention between a seq-reduced query and full-size value and key\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "input = x\n",
        "\n",
        "BertLayer:\n",
        "- BertAttention\n",
        "--- x2 = BertSelfAttention(x)\n",
        "--- x3 = BertSelfOutput(x2,x) -> lnorm(drop(f(x2)) + x)\n",
        "- BertIntermediate (expension:  4*hidden_size)\n",
        "--- x4_ex = activation(f(x3)) # expansion (4*)\n",
        "- BertOutput\n",
        "--- x5 = lnorm(drop(f(x4_ex)) + x3 )\n",
        "\n",
        "\n",
        "inputs = x_l2, x_l3_up\n",
        "\n",
        "BertIntegrativeLayer:\n",
        "- x2 = BertCrossAttention(k,v=x_l2, q=x_l3_up)\n",
        "- x3 = lnorm(drop(f(x2)) + x_l2)\n",
        "- x4_ex = activation( f(cat(x3, x_l3_up))  )\n",
        "- x5 = lnorm(drop(f(x4_ex)) + x3)\n",
        "\n",
        "\n",
        "BertReduceAddIntegrativeLayer\n",
        "inputs = x_l1, x_l1_reduced, x_l2_prev\n",
        "- x2 = BertCrossAttention(k,v=x_l1, q= cat(x_l1_reduced, x_l2_prev) ) -notice three inputs\n",
        "- x3 = lnorm(drop(f(x2)) + x_l2_prev)\n",
        "- x4_ex = activation( f(cat(x3, x_l1_reduced))  )\n",
        "- x5 = lnorm(drop(f(x4_ex)) + x3)\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "class BertReduceAddIntegrativeLayer(nn.Module):\n",
        "    \"\"\"Bert Layer that does dimenion reduction along embedding-dimenion and integrations a skip connection\"\"\"\n",
        "    def __init__(\n",
        "            self, \n",
        "            config, \n",
        "            hidden_size,\n",
        "            hidden_size_input=None,\n",
        "            hidden_size_query=None,\n",
        "            intermediate_size=None,\n",
        "            dim_reduction=2,\n",
        "            do_concat_hidden_and_query = True\n",
        "        ):\n",
        "        super().__init__()\n",
        "        #self.chunk_size_feed_forward = config.chunk_size_feed_forward\n",
        "        #self.seq_len_dim = 1\n",
        "        self.cat = torch.cat\n",
        "        self.do_concat_hidden_and_query = do_concat_hidden_and_query\n",
        "        assert bool(do_concat_hidden_and_query), 'not implemented: concatenation of query and hidden-states must happen'\n",
        "        self.hidden_size = hidden_size\n",
        "        if dim_reduction is None:\n",
        "            dim_reduction = 2\n",
        "        self.dim_reduction = dim_reduction\n",
        "        if intermediate_size is None:\n",
        "            intermediate_size = int(4*hidden_size)\n",
        "        self.intermediate_size = intermediate_size\n",
        "        if hidden_size_input is None:\n",
        "            hidden_size_input = hidden_size\n",
        "        self.hidden_size_input = hidden_size_input\n",
        "        if hidden_size_query is None:\n",
        "            hidden_size_query = hidden_size_input\n",
        "        self.hidden_size_query = hidden_size_query + do_concat_hidden_and_query*hidden_size\n",
        "        self.hidden_size_concat = int(hidden_size + hidden_size_input)\n",
        "\n",
        "        # cross attention between (low-res) query and hidden layers below\n",
        "        self.attention = BertSelfAttnDimensionReduction( \n",
        "            config, \n",
        "            hidden_size_input=self.hidden_size_input, \n",
        "            hidden_size_query = self.hidden_size_query,\n",
        "            position_embedding_type=\"absolute\", \n",
        "            dim_reduction = self.dim_reduction\n",
        "        )\n",
        "        self.is_decoder = config.is_decoder\n",
        "        #inputs = x_l1, x_l1_reduced, x_l2_prev\n",
        "        #- x2 = BertCrossAttention(k,v=x_l1, q= cat(x_l1_reduced, x_l2_prev) ) -notice three inputs\n",
        "        #- x3 = lnorm(drop(f(x2)) + x_l2_prev)\n",
        "        #- x4_ex = activation( f(cat(x3, x_l1_reduced))  )\n",
        "        #- x5 = lnorm(drop(f(x4_ex)) + x3)\n",
        "\n",
        "        # corresponds to BertAttention SelfOutput\n",
        "        self.output_attn = nn.Linear(self.hidden_size, self.hidden_size)\n",
        "        self.lnorm_attn = nn.LayerNorm(self.hidden_size, eps=config.layer_norm_eps)\n",
        "        self.dropout_attn = nn.Dropout(config.hidden_dropout_prob)\n",
        "\n",
        "        # corresponds to BertIntermediate\n",
        "        self.intermediate = nn.Linear(self.hidden_size_concat, self.intermediate_size)\n",
        "        if isinstance(config.hidden_act, str):\n",
        "            self.intermediate_act_fn = ACT2FN[config.hidden_act]\n",
        "        else:\n",
        "            self.intermediate_act_fn = config.hidden_act\n",
        "\n",
        "        # corresponds to BertOutput\n",
        "        self.output_intm = nn.Linear(self.intermediate_size, self.hidden_size)\n",
        "        self.lnorm_intm = nn.LayerNorm(self.hidden_size, eps=config.layer_norm_eps)\n",
        "        self.dropout_intm = nn.Dropout(config.hidden_dropout_prob)\n",
        "    \n",
        "    def forward(\n",
        "        self,\n",
        "        inputs: torch.Tensor, # higher-resolution inputs for key and values (long sequence dimension)\n",
        "        hidden_states: torch.Tensor, # previous hidden-states for skip connection (short squence-dim, low-res)\n",
        "        attention_mask: Optional[torch.FloatTensor] = None,\n",
        "        head_mask: Optional[torch.FloatTensor] = None,\n",
        "        query_hidden_states: torch.FloatTensor = None, # hidden-states for query (short squence-dim, low-res)\n",
        "        query_attention_mask: Optional[torch.FloatTensor] = None,\n",
        "        past_key_value: Optional[Tuple[Tuple[torch.FloatTensor]]] = None,\n",
        "        output_attentions: Optional[bool] = False,\n",
        "    ) -> Tuple[torch.Tensor]:\n",
        "        # decoder uni-directional self-attention cached key/values tuple is at positions 1,2\n",
        "        self_attn_past_key_value = past_key_value[:2] if past_key_value is not None else None\n",
        "\n",
        "        if self.do_concat_hidden_and_query:\n",
        "            query_hidden_states_plus = torch.cat((query_hidden_states, hidden_states),axis=2)\n",
        "        # cross attn between (low-res) query vector and (high-res) key-values\n",
        "        cross_attn_outputs = self.attention(\n",
        "            query_hidden_states_plus, # query (short seq-dim, high-res)\n",
        "            attention_mask=attention_mask,\n",
        "            head_mask=head_mask,\n",
        "            encoder_hidden_states = inputs, # for key/value (longer sequence dimension, high-res)\n",
        "            past_key_value=past_key_value,\n",
        "            output_attentions=output_attentions,\n",
        "        )\n",
        "        cross_hidden_states = cross_attn_outputs[0]\n",
        "\n",
        "        # first Add+Norm skip connection (BertSelfOutput)\n",
        "        cross_hidden_states = self.dropout_attn(self.output_attn(cross_hidden_states))\n",
        "        hidden_states = self.lnorm_attn(cross_hidden_states + hidden_states)\n",
        "\n",
        "        # intermediate expension\n",
        "        intermediate_states = self.intermediate_act_fn(self.intermediate(\n",
        "            self.cat((hidden_states, query_hidden_states),axis=2)\n",
        "        ))\n",
        "        assert intermediate_states.shape[0]==hidden_states.shape[0]\n",
        "        assert intermediate_states.shape[1]==hidden_states.shape[1]\n",
        "\n",
        "        # BertOutput\n",
        "        intermediate_states = self.dropout_intm(self.output_intm(intermediate_states))\n",
        "        out_states = self.lnorm_intm(intermediate_states + hidden_states)\n",
        "\n",
        "        #inputs = x_l1, x_l1_reduced, x_l2_prev\n",
        "        #- x2 = BertCrossAttention(k,v=x_l1, q= cat(x_l1_reduced, x_l2_prev) ) -notice three inputs\n",
        "        #- x3 = lnorm(drop(f(x2)) + x_l2_prev)\n",
        "        #- x4_ex = activation( f(cat(x3, x_l1_reduced))  )\n",
        "        #- x5 = lnorm(drop(f(x4_ex)) + x3)\n",
        "        return out_states\n"
      ],
      "metadata": {
        "id": "ExGOdKwWm_46"
      },
      "execution_count": 254,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# initialize the mid-resolution BertReduceAndIntegrate layer\n",
        "bert_reduce_add_integrate_midres = BertReduceAddIntegrativeLayer(\n",
        "    config, \n",
        "    hidden_size = silo_dimensions[1], # size of mid-res\n",
        "    hidden_size_input=silo_dimensions[0],\n",
        "    hidden_size_query=silo_dimensions[0],\n",
        "    intermediate_size=silo_dimensions[1]*3,\n",
        "    dim_reduction=2,\n",
        "    do_concat_hidden_and_query = True\n",
        ")\n",
        "\n",
        "bert_reduce_add_integrate_lowres = BertReduceAddIntegrativeLayer(\n",
        "    config, \n",
        "    hidden_size = silo_dimensions[2], # size of mid-res\n",
        "    hidden_size_input=silo_dimensions[1],\n",
        "    hidden_size_query=silo_dimensions[1],\n",
        "    intermediate_size=silo_dimensions[2]*3,\n",
        "    dim_reduction=2,\n",
        "    do_concat_hidden_and_query = True\n",
        ")"
      ],
      "metadata": {
        "id": "lkiZo9Npm_xi"
      },
      "execution_count": 256,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Reduce sequence-dim from l1->l2, and from high-res->mid-res\n",
        "hidden_states_hires_reduced = maxpool_l2(hidden_states_hires)\n",
        "assert hidden_states_hires_reduced.shape[1] == hidden_states_midres.shape[1] # reduced-seq-dim should be same as mid-res hidden-states\n",
        "print(hidden_states_midres.shape)\n",
        "hidden_states_midres = bert_reduce_add_integrate_midres(\n",
        "    inputs = hidden_states_hires, # from highres outputs previous layer (key, values)\n",
        "    hidden_states = hidden_states_midres, # previous hidden-states for skip connection (short squence-dim, low-res)\n",
        "    attention_mask = extended_attention_mask_l1_reduced,\n",
        "    head_mask=None,\n",
        "    query_hidden_states = hidden_states_hires_reduced # reduced version of high-res inputs (reduced along sequence dimenion)\n",
        ")\n",
        "print(hidden_states_midres.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gxtTomiPxQn8",
        "outputId": "e252ac72-5287-4652-87fd-4f77c5d135dc"
      },
      "execution_count": 257,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 24, 384])\n",
            "torch.Size([2, 24, 384])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Reduce sequence-dim from l1->l2, and from high-res->mid-res\n",
        "hidden_states_midres_reduced = maxpool_l2(hidden_states_midres)\n",
        "assert hidden_states_midres_reduced.shape[1] == hidden_states_lowres.shape[1] # reduced-seq-dim should be same as mid-res hidden-states\n",
        "print(hidden_states_midres_reduced.shape)\n",
        "\n",
        "if True:\n",
        "  print(hidden_states_lowres.shape)\n",
        "  hidden_states_lowres = bert_reduce_add_integrate_lowres(\n",
        "      inputs = hidden_states_midres, # from highres outputs previous layer (key, values)\n",
        "      hidden_states = hidden_states_lowres, # previous hidden-states for skip connection (short squence-dim, low-res)\n",
        "      attention_mask = extended_attention_mask_l2_reduced,\n",
        "      head_mask=None,\n",
        "      query_hidden_states = hidden_states_midres_reduced # reduced version of high-res inputs (reduced along sequence dimenion)\n",
        "  )\n",
        "  print(hidden_states_lowres.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yPGNtX3KxQuY",
        "outputId": "76ef4023-734a-4e72-a0ad-d6e604bd5e40"
      },
      "execution_count": 258,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 12, 384])\n",
            "torch.Size([2, 12, 192])\n",
            "torch.Size([2, 12, 192])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xJAHT_SyxQwK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Base-Layer nn.Module"
      ],
      "metadata": {
        "id": "qQPJPPkpmibK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModel, AutoTokenizer, AutoConfig\n",
        "import torch\n",
        "from torch import nn\n",
        "from transformers.models.bert.modeling_bert import BertEncoder\n",
        "from transformers.activations import ACT2FN\n",
        "from typing import List, Optional, Tuple, Union\n",
        "\n",
        "def make_config(\n",
        "    modelstring = \"distilroberta-base\",\n",
        "    scale_ratio2 = 0.5,\n",
        "    scale_ratio3 = 0.25,\n",
        "    scale_intermediate2 = 4,\n",
        "    scale_intermediate3 = 4,\n",
        "    num_layers_l3 = 3,\n",
        "    dropout_scaling = 0.05\n",
        "):\n",
        "    #if True:\n",
        "    #modelstring = \"distilroberta-base\"\n",
        "    #scale_ratio2 = 0.5\n",
        "    #scale_ratio3 = 0.25\n",
        "    #scale_intermediate2 = 4\n",
        "    #scale_intermediate3 = 4\n",
        "    base_config = AutoConfig.from_pretrained(modelstring)\n",
        "    config_l3 = copy.deepcopy(base_config)\n",
        "    setattr(base_config, 'model_string', modelstring)\n",
        "    setattr(base_config,'num_layers_l3', num_layers_l3)\n",
        "    setattr(base_config,'scale_ratio2', scale_ratio2)\n",
        "    setattr(base_config,'scale_ratio3', scale_ratio3)\n",
        "    setattr(base_config,'scale_factor2', int(1/base_config.scale_ratio2))\n",
        "    setattr(base_config,'scale_factor3', int(1/base_config.scale_ratio3*base_config.scale_ratio2))\n",
        "    setattr(base_config,\"hidden_size_l2\", int(base_config.hidden_size * scale_ratio2))\n",
        "    setattr(base_config,\"hidden_size_l3\", int(base_config.hidden_size * scale_ratio3))\n",
        "    setattr(base_config,\"intermediate_size_l1\", int(base_config.hidden_size_l2*4.0))\n",
        "    setattr(base_config,\"intermediate_size_l2\", int(base_config.hidden_size_l3*4.0))\n",
        "    setattr(base_config,\"query_size1\", base_config.hidden_size_l2 + base_config.hidden_size_l3)\n",
        "    setattr(base_config,\"query_size2\", base_config.hidden_size_l3)\n",
        "    setattr(base_config,\"dropout_scaling\", dropout_scaling)\n",
        "\n",
        "    # make the configuration for the l3 encoder\n",
        "    config_l3.hidden_size = base_config.hidden_size_l3\n",
        "    config_l3.num_hidden_layers = num_layers_l3\n",
        "    setattr(base_config, 'config_l3', config_l3)\n",
        "    return base_config\n",
        "\n",
        "def initialize_baselayers(config, basemod = None, tokenizer=None):\n",
        "    # initialize the basemodel\n",
        "    if basemod is None:\n",
        "        basemod = AutoModel.from_pretrained(config.model_string)\n",
        "    if tokenizer is None:\n",
        "        # download pretrained tokenizer\n",
        "        tokenizer = AutoTokenizer.from_pretrained(config.model_string)\n",
        "    \n",
        "    device = basemod.device\n",
        "    setattr(config, 'device', device)\n",
        "\n",
        "    # get basemodel's embeddings\n",
        "    layer_embedding = copy.deepcopy(basemod._modules['embeddings'])\n",
        "\n",
        "    # get basemodel's first transformer block\n",
        "    layer_basetransformer = copy.deepcopy(basemod._modules['encoder']._modules['layer']._modules['0'])\n",
        "\n",
        "    # initialize the maxpooling downsamplers\n",
        "    maxpool = nn.Sequential(\n",
        "        nn.Dropout(config.dropout_scaling),\n",
        "        nn.MaxPool2d((2,1), stride=None, padding=0, dilation=1, return_indices=False, ceil_mode=True)\n",
        "    )\n",
        "    # pooling the attention has no dropout\n",
        "    maxpool_attn = nn.MaxPool1d((2), stride=None, padding=0, dilation=1, return_indices=False, ceil_mode=True)\n",
        "\n",
        "    # initialize downsampling attention layers\n",
        "    bert_reducer_l2 = BertSelfAttnDimensionReduction(\n",
        "        config=config,\n",
        "        hidden_size_input=config.hidden_size, \n",
        "        position_embedding_type=config.position_embedding_type,\n",
        "        dim_reduction = config.scale_factor2\n",
        "    )\n",
        "    # 1/4 hidden size\n",
        "    bert_reducer_l3 = BertSelfAttnDimensionReduction(\n",
        "        config=config,\n",
        "        hidden_size_input=config.hidden_size_l2, \n",
        "        position_embedding_type=config.position_embedding_type,\n",
        "        dim_reduction = config.scale_factor3\n",
        "    )\n",
        "\n",
        "    # initialize the low-resolution BertEncoder\n",
        "    bert_encoder_lowres = BertEncoder(config.config_l3)\n",
        "\n",
        "    # initailize the upscalers\n",
        "    upscaler_x2 = InterpolateCombo(scale_factor=config.scale_factor3, dropout=config.dropout_scaling)\n",
        "    upscaler_x4 = InterpolateCombo(scale_factor=int(1/config.scale_ratio3), dropout=config.dropout_scaling)\n",
        "\n",
        "    # initialize the BertIntegrative Layers: low res to mid res\n",
        "    bert_integrative_layer_2 = BertIntegrativeLayer(\n",
        "        config, \n",
        "        hidden_size=config.hidden_size_l2,\n",
        "        hidden_size_query=config.hidden_size_l3,\n",
        "        intermediate_size=config.intermediate_size_l2\n",
        "    )\n",
        "\n",
        "    # from mid-res to high-res\n",
        "    bert_integrative_layer_1 = BertIntegrativeLayer(\n",
        "        config, \n",
        "        hidden_size=config.hidden_size,\n",
        "        hidden_size_query=config.query_size1,\n",
        "        intermediate_size=config.intermediate_size_l1\n",
        "    )\n",
        "\n",
        "    return (\n",
        "        tokenizer,\n",
        "        basemod,\n",
        "        layer_embedding,\n",
        "        layer_basetransformer,\n",
        "        maxpool,\n",
        "        maxpool_attn,\n",
        "        bert_reducer_l2,\n",
        "        bert_reducer_l3,\n",
        "        bert_encoder_lowres,\n",
        "        upscaler_x2,\n",
        "        upscaler_x4,\n",
        "        bert_integrative_layer_2,\n",
        "        bert_integrative_layer_1\n",
        "    )\n",
        "\n",
        "class AnathemBaseModule(nn.Module):\n",
        "    def __init__(\n",
        "            self, \n",
        "            config,\n",
        "            basemod=None,\n",
        "            tokenizer=None,\n",
        "            past_key_values_length = None\n",
        "        ):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "\n",
        "        # initalize the layers\n",
        "        (\n",
        "            tokenizer, basemod, \n",
        "            layer_embedding,\n",
        "            layer_basetransformer,\n",
        "            maxpool,\n",
        "            maxpool_attn,\n",
        "            bert_reducer_l2,\n",
        "            bert_reducer_l3,\n",
        "            bert_encoder_lowres,\n",
        "            upscaler_x2,\n",
        "            upscaler_x4,\n",
        "            bert_integrative_layer_2,\n",
        "            bert_integrative_layer_1\n",
        "        ) = initialize(config, basemod, tokenizer)\n",
        "\n",
        "        self.get_extended_attention_mask = basemod.get_extended_attention_mask\n",
        "        self.embedding = layer_embedding\n",
        "        self.layer_basetransformer = layer_basetransformer\n",
        "        self.maxpool = maxpool\n",
        "        self.maxpool_attn = maxpool_attn\n",
        "        self.bert_reducer_l2 = bert_reducer_l2\n",
        "        self.bert_reducer_l3 = bert_reducer_l3\n",
        "        self.bert_encoder_lowres = bert_encoder_lowres\n",
        "        self.upscaler_x2 = upscaler_x2\n",
        "        self.upscaler_x4 = upscaler_x4\n",
        "        self.bert_integrative_layer_2 = bert_integrative_layer_2\n",
        "        self.bert_integrative_layer_1 = bert_integrative_layer_1\n",
        "    \n",
        "    def forward(\n",
        "        self,\n",
        "        input_ids: Optional[torch.Tensor] = None,\n",
        "        attention_mask: Optional[torch.Tensor] = None,\n",
        "        token_type_ids: Optional[torch.Tensor] = None,\n",
        "        position_ids: Optional[torch.Tensor] = None,\n",
        "        head_mask: Optional[torch.Tensor] = None,\n",
        "        inputs_embeds: Optional[torch.Tensor] = None,\n",
        "        encoder_hidden_states: Optional[torch.Tensor] = None,\n",
        "        encoder_attention_mask: Optional[torch.Tensor] = None,\n",
        "        past_key_values: Optional[List[torch.FloatTensor]] = None,\n",
        "        use_cache: Optional[bool] = None,\n",
        "        output_attentions: Optional[bool] = None,\n",
        "        output_hidden_states: Optional[bool] = None,\n",
        "        return_dict: Optional[bool] = False\n",
        "    ):\n",
        "        input_shape = input_ids\n",
        "        past_key_values_length =0 if past_key_values is None else len(past_key_values)\n",
        "\n",
        "        # extend attention mask\n",
        "        extended_attention_mask_l1 = self.get_extended_attention_mask(attention_mask, input_shape)\n",
        "        # downsample the attention mask to l2 dimension\n",
        "        attention_mask_l2 = self.maxpool_attn(attention_mask.float())\n",
        "        extended_attention_mask_l2 = self.get_extended_attention_mask(attention_mask_l2,attention_mask_l2.shape)\n",
        "        # downsample the attention mask to l3 dimension\n",
        "        attention_mask_l3 = self.maxpool_attn(attention_mask_l2.float())\n",
        "        extended_attention_mask_l3 = self.get_extended_attention_mask(attention_mask_l3,attention_mask_l3.shape)\n",
        "        \n",
        "        # embed\n",
        "        embedding_output = self.embedding(\n",
        "            input_ids = input_ids,\n",
        "            position_ids = position_ids,\n",
        "            token_type_ids = token_type_ids,\n",
        "            #input_embeds=None,\n",
        "            past_key_values_length = past_key_values_length\n",
        "        )\n",
        "\n",
        "        # first transformer block (vanilla transformer)\n",
        "        out_l1 = self.layer_basetransformer(\n",
        "            hidden_states = embedding_output,\n",
        "            attention_mask = extended_attention_mask_l1,\n",
        "            head_mask=head_mask,\n",
        "            encoder_hidden_states=None,\n",
        "            encoder_attention_mask=None,\n",
        "            output_attentions=output_attentions\n",
        "        )\n",
        "        hidden_states_l1 = out_l1[0]\n",
        "\n",
        "        # downsample to sequence 1 to length sequence 2\n",
        "        hiddens_states_l1_reduced = self.maxpool(hidden_states_l1)\n",
        "\n",
        "        # reduce dimenion on sequence 2\n",
        "        out_l2 = self.bert_reducer_l2(\n",
        "            hidden_states = hiddens_states_l1_reduced,\n",
        "            attention_mask = extended_attention_mask_l2,\n",
        "            head_mask=head_mask,\n",
        "            encoder_hidden_states = hidden_states_l1,\n",
        "            encoder_attention_mask= extended_attention_mask_l1,\n",
        "            past_key_value=past_key_values,\n",
        "            output_attentions=output_attentions,\n",
        "        )\n",
        "        hidden_states_l2 = out_l2[0]\n",
        "\n",
        "        # TODO: I should have another vanilla encoder here at this intermediate step\n",
        "        #\n",
        "        # intermediate attention\n",
        "        #\n",
        "        \n",
        "        # reduce sequence length\n",
        "        hiddens_states_l2_reduced = self.maxpool(hidden_states_l2)\n",
        "\n",
        "        # reduce dimenion on sequence 2\n",
        "        out_l3 = self.bert_reducer_l3(\n",
        "            hidden_states = hiddens_states_l2_reduced,\n",
        "            attention_mask = extended_attention_mask_l3,\n",
        "            head_mask=head_mask,\n",
        "            encoder_hidden_states = hidden_states_l2,\n",
        "            encoder_attention_mask= extended_attention_mask_l2,\n",
        "            past_key_value=past_key_values,\n",
        "            output_attentions=output_attentions,\n",
        "        )\n",
        "        hidden_states_l3 = out_l3[0]\n",
        "\n",
        "        #print(hidden_states_l3.shape)\n",
        "        #print(extended_attention_mask_l3.shape)\n",
        "        # BertEncoder at low-res\n",
        "        out_encoder = self.bert_encoder_lowres(\n",
        "            hidden_states=hidden_states_l3,\n",
        "            attention_mask=extended_attention_mask_l3,\n",
        "            head_mask = head_mask,\n",
        "            return_dict=return_dict\n",
        "        )\n",
        "        hidden_states_l3 = out_encoder[0]\n",
        "        \n",
        "        # upscaling: l3 to l2\n",
        "        hidden_states_upscaled3to2 = self.upscaler_x2(hidden_states_l3)\n",
        "\n",
        "        # integrate sequence-2 and upscaled sequence-3\n",
        "        hidden_states_l2 = self.bert_integrative_layer_2(\n",
        "            hidden_states = hidden_states_l2,\n",
        "            attention_mask = extended_attention_mask_l2,\n",
        "            head_mask = head_mask,\n",
        "            query_hidden_states = hidden_states_upscaled3to2,\n",
        "            query_attention_mask = attention_mask_l2\n",
        "        )\n",
        "        \n",
        "        # upscaling: l3/l2 to l1 sequence length\n",
        "        hidden_states_upscaled3to1 = self.upscaler_x4(hidden_states_l3)\n",
        "        hidden_states_upscaled2to1 = self.upscaler_x2(hidden_states_l2)\n",
        "        hidden_states_upscaled = torch.cat((\n",
        "            hidden_states_upscaled2to1, hidden_states_upscaled3to1\n",
        "        ),axis=2)        \n",
        "\n",
        "        # integrate low-resolution information back to original dimension\n",
        "        hidden_states_l1 = self.bert_integrative_layer_1(\n",
        "            hidden_states = hidden_states_l1,\n",
        "            attention_mask = extended_attention_mask_l1,\n",
        "            head_mask = head_mask,\n",
        "            query_hidden_states = hidden_states_upscaled,\n",
        "            query_attention_mask = extended_attention_mask_l1\n",
        "        )\n",
        "        return hidden_states_l1\n",
        "\n",
        "\n",
        "class BertClassificationHead(nn.Module):\n",
        "    def __init__(self, config, n_classes = 1, activation = 'sigmoid'):\n",
        "        super().__init__()\n",
        "        self.dense = nn.Linear(config.hidden_size*2, n_classes)\n",
        "        if activation == 'tanh':\n",
        "            self.activation = nn.Tanh()\n",
        "        elif activation == 'relu':\n",
        "            self.activation = nn.ReLU()\n",
        "        elif activation == 'sigmoid':\n",
        "            self.activation = torch.sigmoid\n",
        "        elif activation == 'none':\n",
        "            self.activation = lambda x: x\n",
        "\n",
        "    def forward(self, hidden_states, attention_mask) -> torch.Tensor:\n",
        "        # We \"pool\" the model by simply taking the hidden state corresponding\n",
        "        # to the first token.\n",
        "        output_vectors=[]\n",
        "        first_token_tensor = hidden_states[:, 0]\n",
        "        output_vectors.append(first_token_tensor)\n",
        "        # mean pooling\n",
        "        input_mask_expanded = attention_mask.unsqueeze(-1).expand(hidden_states.size()).float()\n",
        "        sum_embeddings = torch.sum(hidden_states * input_mask_expanded, 1)\n",
        "        sum_mask = input_mask_expanded.sum(1)\n",
        "        sum_mask = torch.clamp(sum_mask, min=1e-9)\n",
        "        output_vectors.append(sum_embeddings / sum_mask)\n",
        "        # concatenate\n",
        "        pooled_output = torch.concat(output_vectors, axis=1)\n",
        "        #print(pooled_output.shape)\n",
        "        logits = self.dense(pooled_output)\n",
        "        return self.activation(logits)\n",
        "\n",
        "\n",
        "def tokenize_anathem(text):\n",
        "    #padding_length = int(math.ceil(max_length / 4)) * \n",
        "    tokens = tokenizer(text,padding=True, return_tensors='pt')\n",
        "    input_shape = tokens['input_ids'].size()\n",
        "\n",
        "    # change token padding to be multiple of 4\n",
        "    ideal_length = int(math.ceil(input_shape[-1] / 4)) * 4 # should be a multiple of 4\n",
        "    if input_shape[-1]!=ideal_length:\n",
        "      tokens = tokenizer(text,padding='max_length', max_length = ideal_length, return_tensors='pt')\n",
        "      input_shape = tokens['input_ids'].size()\n",
        "\n",
        "    token_type_ids = torch.zeros(input_shape, dtype=torch.long, device=device)\n",
        "    tokens['token_type_ids'] = token_type_ids\n",
        "    \n",
        "    return tokens"
      ],
      "metadata": {
        "id": "vbBLQZJJlu6n"
      },
      "execution_count": 109,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "config = make_config('distilroberta-base')\n",
        "\n",
        "if False:\n",
        "  (tokenizer,basemod,layer_embedding,layer_basetransformer,maxpool,maxpool_attn,bert_reducer_l2,\n",
        "   bert_reducer_l3,bert_encoder_lowres,upscaler_x2,upscaler_x4,bert_integrative_layer_2,bert_integrative_layer_1) = initialize(config)\n",
        "\n",
        "# make the basemod and tokenizer\n",
        "basemod = AutoModel.from_pretrained(config.model_string)\n",
        "tokenizer = AutoTokenizer.from_pretrained(config.model_string)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HdRq7bM3pQhp",
        "outputId": "c1d69c55-c23b-40e2-82a0-5bdbbda3205e"
      },
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at distilroberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.dense.bias']\n",
            "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# the Anathem encoder includes the embeddings and first transformer block\n",
        "anathem_encoder1 = AnathemBaseModule(config, basemod, tokenizer)"
      ],
      "metadata": {
        "id": "pgEOUtHYoB-I"
      },
      "execution_count": 116,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cls_head = BertClassificationHead(config, n_classes = 3, activation = 'none')"
      ],
      "metadata": {
        "id": "87RTY9a059mQ"
      },
      "execution_count": 117,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = [\n",
        "    \"A standard indemnity clause is a waiver clause that states that one party won't hold the other liable for damages, losses, or costs associated with legal issues1.\",\n",
        "    \"It usually consists of two elements: a trigger event or circumstance and a payment obligation2. The trigger event or circumstance is the breach of the agreement, willful misconduct, or negligence of the indemnifying party or its affiliates\"\n",
        "]\n",
        "\n",
        "tokens = tokenize_anathem(text)"
      ],
      "metadata": {
        "id": "gRdY6jEesz0H"
      },
      "execution_count": 118,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "out = anathem_encoder1(\n",
        "      input_ids = tokens['input_ids'],\n",
        "      attention_mask = tokens['attention_mask'],\n",
        "      token_type_ids = tokens['token_type_ids']\n",
        ")\n",
        "cls_head(out, tokens['attention_mask'])\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_A87nQ24Ccoh",
        "outputId": "dd009004-0ca2-47ee-d521-5d3b8173cce4"
      },
      "execution_count": 119,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0.2563,  0.3797, -0.4073],\n",
              "        [ 0.3182,  0.2725, -0.4744]], grad_fn=<AddmmBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 119
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "####"
      ],
      "metadata": {
        "id": "WBi1G7ay63nr"
      },
      "execution_count": 120,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Next steps, do something simple like sentiment analysis"
      ],
      "metadata": {
        "id": "F2L-jYJ6u4qd"
      },
      "execution_count": 121,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import list_datasets, load_dataset\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from torch.optim import AdamW\n",
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "from scipy.special import softmax\n",
        "#datasets_list = list_datasets()\n",
        "#[k for k in datasets_list if 'phrasebank' in k]\n"
      ],
      "metadata": {
        "id": "cLHCo5vZ-brJ"
      },
      "execution_count": 122,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "[k for k in datasets_list if 'phrasebank' in k]\n",
        "\n",
        "dataset = load_dataset('financial_phrasebank', 'sentences_75agree')\n",
        "\n",
        "# split\n",
        "idx_train, idx_val = train_test_split(np.arange(len(dataset['train']['sentence'])), test_size=0.1)\n",
        "dataset_train = [{'text':dataset['train']['sentence'][idx], 'label':dataset['train']['label'][idx]}  for idx in idx_train]\n",
        "dataset_val = [{'text':dataset['train']['sentence'][idx], 'label':dataset['train']['label'][idx]} for idx in idx_val]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66,
          "referenced_widgets": [
            "214dc90a0e654400afc472016e0b7c9a",
            "c2ce41185c3046948178700750de15b5",
            "eb4c10cd3e8841a78e96cd4fa4024700",
            "9150f3d3ea024b248be3f6bd0e51d57a",
            "39d0d3b4c43f49e2b05a6e8f188a1c57",
            "8444b4869f6246b6bb77ce85d55fc75a",
            "92bf67b384a445bc814c611c5e524c7a",
            "5c6cd07ab4bf408aaed547bc40a849d7",
            "f4209cb165154eadaab1c10da0ec1677",
            "3a6b904c8a99465e90a2e2710a27191d",
            "4942f87d85aa4a45b7c2c8f797b1f511"
          ]
        },
        "id": "rvjAv19p1Un6",
        "outputId": "dd37e5f9-2533-4b35-8789-e80c0b2b8cd9"
      },
      "execution_count": 123,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:datasets.builder:Found cached dataset financial_phrasebank (/root/.cache/huggingface/datasets/financial_phrasebank/sentences_75agree/1.0.0/550bde12e6c30e2674da973a55f57edde5181d53f5a5a34c1531c53f93b7e141)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/1 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "214dc90a0e654400afc472016e0b7c9a"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(dataset_train)); print(len(dataset_val))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q_fRokuwB1h9",
        "outputId": "60de1c58-2bb8-4601-db72-e1b8e53e9a02"
      },
      "execution_count": 124,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3107\n",
            "346\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class MyDataset(Dataset):\n",
        "    \"\"\"torch dataset.\"\"\"\n",
        "\n",
        "    def __init__(self, dataset):\n",
        "        self.data = dataset\n",
        "        self.n = len(self.data)\n",
        "    \n",
        "    def __len__(self):\n",
        "        return self.n\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if torch.is_tensor(idx):\n",
        "            idx = idx.tolist()\n",
        "        unit = self.data[idx]\n",
        "        return unit"
      ],
      "metadata": {
        "id": "8B86BY_m_x12"
      },
      "execution_count": 125,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ds_train = MyDataset(dataset_train)\n",
        "ds_val = MyDataset(dataset_val)"
      ],
      "metadata": {
        "id": "uh3NKKrZ_xuX"
      },
      "execution_count": 126,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size_train = 12\n",
        "batch_size_val = 36\n",
        "lr = 0.0001\n",
        "eval_iter = 100\n",
        "n_epochs = 5"
      ],
      "metadata": {
        "id": "xn4rR8QqEiS3"
      },
      "execution_count": 169,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dl_train = DataLoader(ds_train, batch_size=batch_size_train, shuffle=True)\n",
        "dl_val = DataLoader(ds_val, batch_size=batch_size_val, shuffle=False)"
      ],
      "metadata": {
        "id": "t9JPB6miBpQ9"
      },
      "execution_count": 135,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = AdamW(list(anathem_encoder1.parameters())+ list(cls_head.parameters()), lr=lr)"
      ],
      "metadata": {
        "id": "WbhvjxbeKHh9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "optimizer.zero_grad()\n",
        "anathem_encoder1.train()\n",
        "cls_head.train()\n",
        "for epoch in range(n_epochs):\n",
        "\n",
        "  for iteration, batch in enumerate(tqdm(dl_train, disable=True)):\n",
        "      \n",
        "      # tokenize the batch\n",
        "      tokens = tokenize_anathem(batch['text'])\n",
        "      target = batch['label']\n",
        "      \n",
        "      optimizer.zero_grad()\n",
        "\n",
        "      features = anathem_encoder1(\n",
        "        input_ids = tokens['input_ids'],\n",
        "        attention_mask = tokens['attention_mask'],\n",
        "        token_type_ids = tokens['token_type_ids']\n",
        "      )\n",
        "\n",
        "      # prediction\n",
        "      preds = cls_head(features, tokens['attention_mask'])\n",
        "\n",
        "      # loss\n",
        "      loss = nn.functional.cross_entropy(preds, target)\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "      # do evaluation\n",
        "      if ((i+1) % eval_iter)==0:  \n",
        "          anathem_encoder1.eval()\n",
        "          cls_head.eval()\n",
        "          # tokenize the eval\n",
        "          eval_logits = []\n",
        "          eval_targets = []\n",
        "          for i, batch_eval in enumerate(tqdm(dl_val, disable=True)):\n",
        "              with torch.no_grad():\n",
        "                  # tokenize the batch\n",
        "                  tokens_eval = tokenize_anathem(batch_eval['text'])\n",
        "                  labels_eval = batch_eval['label']\n",
        "                  features = anathem_encoder1(\n",
        "                      input_ids = tokens_eval['input_ids'],\n",
        "                      attention_mask = tokens_eval['attention_mask'],\n",
        "                      token_type_ids = tokens_eval['token_type_ids']\n",
        "                  )\n",
        "                  # prediction\n",
        "                  batch_logits = cls_head(features, tokens_eval['attention_mask'])\n",
        "                  eval_logits+=batch_logits.detach().tolist()\n",
        "                  eval_targets+=labels_eval.detach().tolist()\n",
        "\n",
        "          eval_prec,eval_recall,eval_f1,eval_support = precision_recall_fscore_support(eval_targets, np.array(eval_logits).argmax(axis=1),zero_division=0)\n",
        "          print('E:%d; i:%d: f1:%0.3f (%0.3f); prec:%0.3f (%0.3f); rec:%0.3f (%0.3f)' % (epoch, iteration, eval_f1.mean(), eval_f1.min(), eval_prec.mean(), eval_prec.min(), eval_recall.mean(), eval_recall.min()))\n",
        "          cls_head.train()\n",
        "          anathem_encoder1.train()\n",
        "\n",
        "          \n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 503
        },
        "id": "fPmQOT5OEnEY",
        "outputId": "5e98f72b-0f5e-4bec-aa00-bbc24e51302d"
      },
      "execution_count": 173,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E:0; i:14: f1:0.794 (0.716); prec:0.793 (0.644); rec:0.807 (0.674)\n",
            "E:0; i:14: f1:0.841 (0.767); prec:0.832 (0.660); rec:0.868 (0.739)\n",
            "E:1; i:14: f1:0.905 (0.880); prec:0.897 (0.846); rec:0.914 (0.880)\n",
            "E:1; i:14: f1:0.878 (0.819); prec:0.864 (0.723); rec:0.904 (0.837)\n",
            "E:2; i:14: f1:0.917 (0.886); prec:0.930 (0.912); rec:0.906 (0.861)\n",
            "E:2; i:14: f1:0.885 (0.833); prec:0.887 (0.833); rec:0.882 (0.833)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-173-4f754a4aaab1>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m       \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_entropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m       \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m       \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m       \u001b[0;31m# do evaluation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    278\u001b[0m                                                f\"but got {result}.\")\n\u001b[1;32m    279\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 280\u001b[0;31m                 \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    281\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_optimizer_step_code\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    282\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36m_use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_grad_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefaults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'differentiable'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m             \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_grad_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprev_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    169\u001b[0m             )\n\u001b[1;32m    170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 171\u001b[0;31m             adamw(\n\u001b[0m\u001b[1;32m    172\u001b[0m                 \u001b[0mparams_with_grad\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m                 \u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py\u001b[0m in \u001b[0;36madamw\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    319\u001b[0m         \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_single_tensor_adamw\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 321\u001b[0;31m     func(\n\u001b[0m\u001b[1;32m    322\u001b[0m         \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m         \u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py\u001b[0m in \u001b[0;36m_single_tensor_adamw\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[1;32m    438\u001b[0m                 \u001b[0mdenom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmax_exp_avg_sqs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mbias_correction2_sqrt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    439\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 440\u001b[0;31m                 \u001b[0mdenom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mbias_correction2_sqrt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    441\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m             \u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddcdiv_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexp_avg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdenom\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mstep_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# do evaluation\n",
        "if True:\n",
        "    if True:\n",
        "        if True: #if (i+1) % eval_iter:\n",
        "            anathem_encoder1.eval()\n",
        "            cls_head.eval()\n",
        "            # tokenize the eval\n",
        "            eval_logits = []\n",
        "            eval_targets = []\n",
        "            for i, batch in enumerate(tqdm(dl_val, disable=True)):\n",
        "                with torch.no_grad():\n",
        "                    # tokenize the batch\n",
        "                    tokens = tokenize_anathem(batch['text'])\n",
        "                    target = batch['label']\n",
        "                    features = anathem_encoder1(\n",
        "                      input_ids = tokens['input_ids'],\n",
        "                      attention_mask = tokens['attention_mask'],\n",
        "                      token_type_ids = tokens['token_type_ids']\n",
        "                    )\n",
        "                    # prediction\n",
        "                    batch_logits = cls_head(features, tokens['attention_mask'])\n",
        "                    eval_logits+=batch_logits.detach().tolist()\n",
        "                    eval_targets+=target.detach().tolist()\n",
        "\n",
        "            eval_prec,eval_recall,eval_f1,eval_support = precision_recall_fscore_support(eval_targets, np.array(eval_logits).argmax(axis=1),zero_division=0)\n",
        "            print('E:%d; i:%d: f1:%0.3f (%0.3f); prec:%0.3f (%0.3f); rec:%0.3f (%0.3f)' % (epoch, i, eval_f1.mean(), eval_f1.min(), eval_prec.mean(), eval_prec.min(), eval_recall.mean(), eval_recall.min()))\n",
        "            cls_head.train()\n",
        "            anathem_encoder1.train()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XiKeaFEQKLUg",
        "outputId": "01b1dc02-0c04-4245-c653-679085d69d4a"
      },
      "execution_count": 160,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E:%d; i:%d: f1:%0.3f; rec: %0.3f; prec:%0.3f 0 14 [0.         0.86464646 0.52173913] [0.         0.98165138 0.45652174] [0.         0.77256318 0.60869565]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('E:%d; i:%d: f1:%0.3f (%0.3f); prec:%0.3f (%0.3f); rec:%0.3f (%0.3f)' % (epoch, i, eval_f1.mean(), eval_f1.min(), eval_prec.mean(), eval_prec.min(), eval_recall.mean(), eval_recall.min()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AaHOImksIE4s",
        "outputId": "19eadf5c-d753-48e3-fee4-771840825b5c"
      },
      "execution_count": 168,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E:0; i:14: f1:0.462 (0.000); prec:0.460 (0.000); rec:0.479 (0.000)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "eval_f1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RpDJu4TrGJme",
        "outputId": "3223ab06-d6db-4c25-da03-b33c72689884"
      },
      "execution_count": 167,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.        , 0.86464646, 0.52173913])"
            ]
          },
          "metadata": {},
          "execution_count": 167
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "eval_prec,eval_recall,eval_f1,eval_support = precision_recall_fscore_support(eval_targets, np.array(eval_logits).argmax(axis=1),zero_division=0)"
      ],
      "metadata": {
        "id": "3kO-XYGLFCuk"
      },
      "execution_count": 159,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LyA6IEvtFKcz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}